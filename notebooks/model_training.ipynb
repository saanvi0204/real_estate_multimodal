{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6151857",
   "metadata": {},
   "source": [
    "### Phase 4 — Multimodal Fusion Modeling & Final Evaluation\n",
    "\n",
    "#### Objective\n",
    "Evaluate and compare multiple multimodal fusion strategies for property price prediction by combining tabular features and satellite image representations. This notebook systematically benchmarks different fusion approaches and selects the final model based on empirical performance.\n",
    "\n",
    "#### Fusion Strategies Evaluated\n",
    "- Tabular-only baseline (XGBoost)\n",
    "- Late fusion (neural network)\n",
    "- Early fusion (neural network)\n",
    "- Two-stage hybrid fusion (tree-based)\n",
    "- Residual fusion\n",
    "- Stacked generalization (meta-learning)\n",
    "\n",
    "#### Key Steps\n",
    "- Load tabular features and image embeddings\n",
    "- Train and evaluate each fusion strategy\n",
    "- Compare models using RMSE and R² metrics\n",
    "- Select the final model architecture based on validation performance\n",
    "\n",
    "#### Output\n",
    "- Performance comparison table\n",
    "- Justification for the chosen final multimodal model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821f4eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "TABULAR_PATH = \"../data/processed/train_clean.csv\"\n",
    "EMB_PATH = \"../data/processed/image_embeddings.npy\"\n",
    "EMB_ID_PATH = \"../data/processed/image_ids.npy\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 40\n",
    "LR = 1e-3\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab8ea8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned shape: (16406, 529)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(TABULAR_PATH)\n",
    "\n",
    "img_emb = np.load(EMB_PATH)\n",
    "img_ids = np.load(EMB_ID_PATH)\n",
    "\n",
    "emb_df = pd.DataFrame(img_emb)\n",
    "emb_df[\"id\"] = img_ids\n",
    "\n",
    "df = df.merge(emb_df, on=\"id\", how=\"inner\")\n",
    "print(\"Aligned shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de6f500a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17622979762522115"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training XGBoost on image embeddings only to check their quality\n",
    "X_img = df.iloc[:, -img_emb.shape[1]:].values\n",
    "y_img = df[\"log_price\"].values\n",
    "\n",
    "xgb_img = XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_img.fit(X_img, y_img)\n",
    "\n",
    "preds = xgb_img.predict(X_img)\n",
    "\n",
    "rmse = root_mean_squared_error(\n",
    "    y_img,\n",
    "    preds,\n",
    ")\n",
    "\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae286212",
   "metadata": {},
   "source": [
    "Satellite imagery alone provides good predictive signal (RMSE ≈ 0.18), indicating that environmental context contains meaningful information about property value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9141b2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TABULAR_FEATURES = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n",
    "       'waterfront', 'view', 'condition', 'grade', 'lat', 'long',\n",
    "       'sqft_living15', 'sqft_lot15', 'size_quality', 'living_density_ratio']\n",
    "\n",
    "X_tab = df[TABULAR_FEATURES].values\n",
    "X_img = df.iloc[:, -img_emb.shape[1]:].values\n",
    "y = df[\"log_price\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d502c6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tab_train, X_tab_val, X_img_train, X_img_val, y_train, y_val = train_test_split(\n",
    "    X_tab, X_img, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06b45bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_scaler = StandardScaler()\n",
    "X_tab_train = tab_scaler.fit_transform(X_tab_train)\n",
    "X_tab_val = tab_scaler.transform(X_tab_val)\n",
    "\n",
    "img_scaler = StandardScaler()\n",
    "X_img_train = img_scaler.fit_transform(X_img_train)\n",
    "X_img_val = img_scaler.transform(X_img_val)\n",
    "\n",
    "pca = PCA(n_components=32, random_state=42)\n",
    "X_img_train_nn = pca.fit_transform(X_img_train)\n",
    "X_img_val_nn = pca.transform(X_img_val)\n",
    "\n",
    "IMG_DIM = X_img_train_nn.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6817a0",
   "metadata": {},
   "source": [
    "#### Tabular Baseline (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eea27274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.17074308510789515, 0.8949758774393711)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab_model = XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "tab_model.fit(X_tab_train, y_train)\n",
    "tab_preds = tab_model.predict(X_tab_val)\n",
    "\n",
    "rmse_tab = root_mean_squared_error(y_val, tab_preds)\n",
    "r2_tab = r2_score(y_val, tab_preds)\n",
    "\n",
    "rmse_tab, r2_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0789706f",
   "metadata": {},
   "source": [
    "#### Late Fusion Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "665692b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LateFusionModel(nn.Module):\n",
    "    def __init__(self, tab_dim, img_dim):\n",
    "        super().__init__()\n",
    "        self.tab = nn.Sequential(\n",
    "            nn.Linear(tab_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "        self.img = nn.Sequential(\n",
    "            nn.Linear(img_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 32)\n",
    "        )\n",
    "        self.reg = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, xt, xi):\n",
    "        return self.reg(torch.cat([self.tab(xt), self.img(xi)], dim=1)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd66903",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(\n",
    "    torch.tensor(X_tab_train, dtype=torch.float32),\n",
    "    torch.tensor(X_img_train_nn, dtype=torch.float32),\n",
    "    torch.tensor(y_train, dtype=torch.float32)\n",
    ")\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "model = LateFusionModel(X_tab_train.shape[1], IMG_DIM).to(DEVICE) \n",
    "opt = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-4) \n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1526f214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 20.2788\n",
      "Epoch 2 | Train Loss: 1.7654\n",
      "Epoch 3 | Train Loss: 0.8981\n",
      "Epoch 4 | Train Loss: 0.4102\n",
      "Epoch 5 | Train Loss: 0.1951\n",
      "Epoch 6 | Train Loss: 0.1169\n",
      "Epoch 7 | Train Loss: 0.0900\n",
      "Epoch 8 | Train Loss: 0.0760\n",
      "Epoch 9 | Train Loss: 0.0686\n",
      "Epoch 10 | Train Loss: 0.0608\n",
      "Epoch 11 | Train Loss: 0.0571\n",
      "Epoch 12 | Train Loss: 0.0558\n",
      "Epoch 13 | Train Loss: 0.0529\n",
      "Epoch 14 | Train Loss: 0.0511\n",
      "Epoch 15 | Train Loss: 0.0481\n",
      "Epoch 16 | Train Loss: 0.0483\n",
      "Epoch 17 | Train Loss: 0.0466\n",
      "Epoch 18 | Train Loss: 0.0435\n",
      "Epoch 19 | Train Loss: 0.0447\n",
      "Epoch 20 | Train Loss: 0.0464\n",
      "Epoch 21 | Train Loss: 0.0426\n",
      "Epoch 22 | Train Loss: 0.0410\n",
      "Epoch 23 | Train Loss: 0.0467\n",
      "Epoch 24 | Train Loss: 0.0409\n",
      "Epoch 25 | Train Loss: 0.0424\n",
      "Epoch 26 | Train Loss: 0.0442\n",
      "Epoch 27 | Train Loss: 0.0418\n",
      "Epoch 28 | Train Loss: 0.0482\n",
      "Epoch 29 | Train Loss: 0.0452\n",
      "Epoch 30 | Train Loss: 0.0401\n",
      "Epoch 31 | Train Loss: 0.0408\n",
      "Epoch 32 | Train Loss: 0.0383\n",
      "Epoch 33 | Train Loss: 0.0363\n",
      "Epoch 34 | Train Loss: 0.0370\n",
      "Epoch 35 | Train Loss: 0.0388\n",
      "Epoch 36 | Train Loss: 0.0380\n",
      "Epoch 37 | Train Loss: 0.0368\n",
      "Epoch 38 | Train Loss: 0.0381\n",
      "Epoch 39 | Train Loss: 0.0437\n",
      "Epoch 40 | Train Loss: 0.0361\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.21436719254998202, 0.8344537094599237)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for xt, xi, y in loader:\n",
    "        xt, xi, y = xt.to(DEVICE), xi.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        preds = model(xt, xi)\n",
    "        loss = loss_fn(preds, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {epoch_loss/len(loader):.4f}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    nn_preds = model(\n",
    "        torch.tensor(X_tab_val, dtype=torch.float32).to(DEVICE),\n",
    "        torch.tensor(X_img_val_nn, dtype=torch.float32).to(DEVICE)\n",
    "    ).cpu().numpy()\n",
    "\n",
    "rmse_late = root_mean_squared_error(y_val, nn_preds)\n",
    "r2_late = r2_score(y_val, nn_preds)\n",
    "\n",
    "rmse_late, r2_late"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89668950",
   "metadata": {},
   "source": [
    "#### Early Fusion Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d46fa40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyFusionModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c13046f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_early = np.hstack([X_tab_train, X_img_train_nn])\n",
    "X_val_early = np.hstack([X_tab_val, X_img_val_nn])\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(X_train_early, dtype=torch.float32),\n",
    "    torch.tensor(y_train, dtype=torch.float32)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "model_early = EarlyFusionModel(X_train_early.shape[1]).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model_early.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87e1c417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 | Train Loss: 20.9034\n",
      "Epoch 2/40 | Train Loss: 2.8816\n",
      "Epoch 3/40 | Train Loss: 2.1501\n",
      "Epoch 4/40 | Train Loss: 1.6333\n",
      "Epoch 5/40 | Train Loss: 1.2337\n",
      "Epoch 6/40 | Train Loss: 0.9274\n",
      "Epoch 7/40 | Train Loss: 0.6753\n",
      "Epoch 8/40 | Train Loss: 0.4608\n",
      "Epoch 9/40 | Train Loss: 0.3236\n",
      "Epoch 10/40 | Train Loss: 0.2369\n",
      "Epoch 11/40 | Train Loss: 0.1725\n",
      "Epoch 12/40 | Train Loss: 0.1426\n",
      "Epoch 13/40 | Train Loss: 0.1209\n",
      "Epoch 14/40 | Train Loss: 0.1097\n",
      "Epoch 15/40 | Train Loss: 0.1028\n",
      "Epoch 16/40 | Train Loss: 0.0971\n",
      "Epoch 17/40 | Train Loss: 0.0879\n",
      "Epoch 18/40 | Train Loss: 0.0810\n",
      "Epoch 19/40 | Train Loss: 0.0777\n",
      "Epoch 20/40 | Train Loss: 0.0789\n",
      "Epoch 21/40 | Train Loss: 0.0732\n",
      "Epoch 22/40 | Train Loss: 0.0724\n",
      "Epoch 23/40 | Train Loss: 0.0729\n",
      "Epoch 24/40 | Train Loss: 0.0720\n",
      "Epoch 25/40 | Train Loss: 0.0718\n",
      "Epoch 26/40 | Train Loss: 0.0659\n",
      "Epoch 27/40 | Train Loss: 0.0647\n",
      "Epoch 28/40 | Train Loss: 0.0656\n",
      "Epoch 29/40 | Train Loss: 0.0716\n",
      "Epoch 30/40 | Train Loss: 0.0615\n",
      "Epoch 31/40 | Train Loss: 0.0636\n",
      "Epoch 32/40 | Train Loss: 0.0598\n",
      "Epoch 33/40 | Train Loss: 0.0594\n",
      "Epoch 34/40 | Train Loss: 0.0569\n",
      "Epoch 35/40 | Train Loss: 0.0558\n",
      "Epoch 36/40 | Train Loss: 0.0562\n",
      "Epoch 37/40 | Train Loss: 0.0537\n",
      "Epoch 38/40 | Train Loss: 0.0525\n",
      "Epoch 39/40 | Train Loss: 0.0532\n",
      "Epoch 40/40 | Train Loss: 0.0510\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.29251761839223694, 0.6917473130232774)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model_early.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        yb = yb.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model_early(xb)\n",
    "        loss = loss_fn(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {epoch_loss / len(train_loader):.4f}\")\n",
    "\n",
    "rmse_early = root_mean_squared_error(\n",
    "    y_val,\n",
    "    model_early(torch.tensor(X_val_early, dtype=torch.float32).to(DEVICE)).cpu().detach().numpy(),\n",
    ")\n",
    "\n",
    "r2_early = r2_score(\n",
    "    y_val,\n",
    "    model_early(torch.tensor(X_val_early, dtype=torch.float32).to(DEVICE)).cpu().detach().numpy()\n",
    ")\n",
    "\n",
    "rmse_early, r2_early"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9b0c38",
   "metadata": {},
   "source": [
    "#### Two-Stage Hybrid (Tree-Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcda52c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.16584030156480697, 0.9009206905878419)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_hybrid_train = np.hstack([X_tab_train, X_img_train])\n",
    "X_hybrid_val = np.hstack([X_tab_val, X_img_val])\n",
    "\n",
    "hybrid_model = XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "hybrid_model.fit(X_hybrid_train, y_train)\n",
    "hybrid_preds = hybrid_model.predict(X_hybrid_val)\n",
    "\n",
    "rmse_hybrid = root_mean_squared_error(y_val, hybrid_preds)\n",
    "r2_hybrid = r2_score(y_val, hybrid_preds)\n",
    "\n",
    "rmse_hybrid, r2_hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db29e2d6",
   "metadata": {},
   "source": [
    "#### Residual Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ecd16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_residuals = np.zeros_like(y_train)\n",
    "\n",
    "for tr_idx, va_idx in kf.split(X_tab_train):\n",
    "    X_tr, X_va = X_tab_train[tr_idx], X_tab_train[va_idx]\n",
    "    y_tr, y_va = y_train[tr_idx], y_train[va_idx]\n",
    "\n",
    "    tab_cv = XGBRegressor(\n",
    "        n_estimators=500,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "    tab_cv.fit(X_tr, y_tr)\n",
    "    oof_residuals[va_idx] = y_va - tab_cv.predict(X_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94aba81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.15535057527169696, 0.9130582165147851)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residual_train = oof_residuals \n",
    "residual_val = y_val - tab_preds\n",
    "\n",
    "res_model = XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "res_model.fit(X_img_train, residual_train)\n",
    "res_preds= res_model.predict(X_img_val)\n",
    "\n",
    "final_preds = tab_preds + res_preds\n",
    "rmse_res = root_mean_squared_error(y_val, final_preds)\n",
    "r2_res = r2_score(y_val, final_preds)\n",
    "rmse_res, r2_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1952d4a9",
   "metadata": {},
   "source": [
    "#### Stacked Generalization (Meta-Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "449094de",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_only_model = XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "img_only_model.fit(X_img_train, y_train)\n",
    "\n",
    "img_preds = img_only_model.predict(X_img_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47a7f3d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.23407638692273325, 0.8026132375191068)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_X_train = np.column_stack([\n",
    "    tab_model.predict(X_tab_train),\n",
    "    img_only_model.predict(X_img_train)\n",
    "])\n",
    "\n",
    "meta_X_val = np.column_stack([\n",
    "    tab_preds,\n",
    "    img_preds\n",
    "])\n",
    "\n",
    "meta_model = LinearRegression()\n",
    "meta_model.fit(meta_X_train, y_train)\n",
    "\n",
    "stacked_preds = meta_model.predict(meta_X_val)\n",
    "\n",
    "rmse_stacked = root_mean_squared_error(y_val, stacked_preds)\n",
    "r2_stacked = r2_score(y_val, stacked_preds)\n",
    "\n",
    "rmse_stacked, r2_stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6410e9c",
   "metadata": {},
   "source": [
    "#### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08a5c1b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Residual Fusion</td>\n",
       "      <td>0.155351</td>\n",
       "      <td>0.913058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hybrid Fusion (XGB)</td>\n",
       "      <td>0.165840</td>\n",
       "      <td>0.900921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tabular (XGB)</td>\n",
       "      <td>0.170743</td>\n",
       "      <td>0.894976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Late Fusion (NN)</td>\n",
       "      <td>0.214367</td>\n",
       "      <td>0.834454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stacked Generalization</td>\n",
       "      <td>0.234076</td>\n",
       "      <td>0.802613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Early Fusion (NN)</td>\n",
       "      <td>0.292518</td>\n",
       "      <td>0.691747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model      RMSE        R2\n",
       "4         Residual Fusion  0.155351  0.913058\n",
       "3     Hybrid Fusion (XGB)  0.165840  0.900921\n",
       "0           Tabular (XGB)  0.170743  0.894976\n",
       "1        Late Fusion (NN)  0.214367  0.834454\n",
       "5  Stacked Generalization  0.234076  0.802613\n",
       "2       Early Fusion (NN)  0.292518  0.691747"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({\n",
    "    \"Model\": [\n",
    "        \"Tabular (XGB)\",\n",
    "        \"Late Fusion (NN)\",\n",
    "        \"Early Fusion (NN)\",\n",
    "        \"Hybrid Fusion (XGB)\",\n",
    "        \"Residual Fusion\",\n",
    "        \"Stacked Generalization\"\n",
    "    ],\n",
    "    \"RMSE\": [\n",
    "        rmse_tab,\n",
    "        rmse_late,\n",
    "        rmse_early,\n",
    "        rmse_hybrid,\n",
    "        rmse_res,\n",
    "        rmse_stacked\n",
    "    ],\n",
    "    \"R2\": [\n",
    "        r2_tab,\n",
    "        r2_late,\n",
    "        r2_early,\n",
    "        r2_hybrid,\n",
    "        r2_res,\n",
    "        r2_stacked\n",
    "    ]\n",
    "})\n",
    "\n",
    "results.sort_values(\"RMSE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
